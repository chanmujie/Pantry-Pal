{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxCcWsq7fr7K"
   },
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2NkNBjxqY8f"
   },
   "source": [
    "## 1.1 Load necessary libraries and read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "La0B9Hr71a1K"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "collapsed": true,
    "id": "WpqwB8XimgSa",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ee3e3eba-01c7-4ba0-ac8d-76f8aa4c631e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 494963 entries, 0 to 494962\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   id                   494963 non-null  int64 \n",
      " 1   name                 494963 non-null  object\n",
      " 2   description          485362 non-null  object\n",
      " 3   ingredients          494963 non-null  object\n",
      " 4   ingredients_raw_str  494963 non-null  object\n",
      " 5   serving_size         494963 non-null  object\n",
      " 6   servings             494963 non-null  int64 \n",
      " 7   steps                494963 non-null  object\n",
      " 8   tags                 494963 non-null  object\n",
      " 9   search_terms         494963 non-null  object\n",
      "dtypes: int64(2), object(8)\n",
      "memory usage: 37.8+ MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-5c23f22d-e94e-4593-a695-a192f03ecc39\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_raw_str</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>servings</th>\n",
       "      <th>steps</th>\n",
       "      <th>tags</th>\n",
       "      <th>search_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96313</td>\n",
       "      <td>Grilled Garlic Cheese Grits</td>\n",
       "      <td>We love grits, this is another good way to ser...</td>\n",
       "      <td>['water', 'grits', 'salt', 'cheddar cheese', '...</td>\n",
       "      <td>[\"4   cups    water\",\"1   cup   uncooked old f...</td>\n",
       "      <td>1 (155 g)</td>\n",
       "      <td>8</td>\n",
       "      <td>['I a sauce pan, bring water to a boil; slowly...</td>\n",
       "      <td>['time-to-make', 'course', 'main-ingredient', ...</td>\n",
       "      <td>{'diabetic', 'low-calorie', 'vegetarian', 'low...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232037</td>\n",
       "      <td>Simple Shrimp and Andouille Jambalaya</td>\n",
       "      <td>Simple, easy and very tasty for when you are i...</td>\n",
       "      <td>['onion', 'red bell pepper', 'garlic cloves', ...</td>\n",
       "      <td>[\"1   medium    onion, chopped coarse \",\"1   m...</td>\n",
       "      <td>1 (366 g)</td>\n",
       "      <td>4</td>\n",
       "      <td>['In a food processor, pulse the onion, red pe...</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>{'dinner', 'shrimp'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41090</td>\n",
       "      <td>black-and-white bean salad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['white beans', 'canned black beans', 'tomatoe...</td>\n",
       "      <td>[\"1   cup   canned white beans, rinsed and dra...</td>\n",
       "      <td>1 (807 g)</td>\n",
       "      <td>1</td>\n",
       "      <td>['In a large bowl, combine beans, tomato, onio...</td>\n",
       "      <td>['15-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>{'vegetarian', 'salad', 'side', 'dinner', 'veg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60656</td>\n",
       "      <td>Crock Pot Italian Zucchini</td>\n",
       "      <td>This is a good recipe for weight watchers. It ...</td>\n",
       "      <td>['zucchini', 'yellow squash', 'diced tomatoes'...</td>\n",
       "      <td>[\"2       zucchini, sliced \",\"2   small    yel...</td>\n",
       "      <td>1 (244 g)</td>\n",
       "      <td>4</td>\n",
       "      <td>['Put all ingredients in the crock pot and coo...</td>\n",
       "      <td>['weeknight', 'time-to-make', 'course', 'main-...</td>\n",
       "      <td>{'side', 'vegetarian', 'italian'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>232047</td>\n",
       "      <td>Beef Stew With Dried Cherries</td>\n",
       "      <td>This is a fabulous stew that came from one of ...</td>\n",
       "      <td>['beef stew meat', 'flour', 'salt', 'allspice'...</td>\n",
       "      <td>[\"3   lbs    beef stew meat\",\"3   tablespoons ...</td>\n",
       "      <td>1 (358 g)</td>\n",
       "      <td>8</td>\n",
       "      <td>['Preheat oven to 350Â°F.', \"Cut beef into 1 in...</td>\n",
       "      <td>['time-to-make', 'course', 'main-ingredient', ...</td>\n",
       "      <td>{'dinner'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c23f22d-e94e-4593-a695-a192f03ecc39')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5c23f22d-e94e-4593-a695-a192f03ecc39 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5c23f22d-e94e-4593-a695-a192f03ecc39');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-81fa3be8-0265-48fe-bd2e-2908f2a9b31f\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-81fa3be8-0265-48fe-bd2e-2908f2a9b31f')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-81fa3be8-0265-48fe-bd2e-2908f2a9b31f button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       id                                   name  \\\n",
       "0   96313            Grilled Garlic Cheese Grits   \n",
       "1  232037  Simple Shrimp and Andouille Jambalaya   \n",
       "2   41090             black-and-white bean salad   \n",
       "3   60656             Crock Pot Italian Zucchini   \n",
       "4  232047          Beef Stew With Dried Cherries   \n",
       "\n",
       "                                         description  \\\n",
       "0  We love grits, this is another good way to ser...   \n",
       "1  Simple, easy and very tasty for when you are i...   \n",
       "2                                                NaN   \n",
       "3  This is a good recipe for weight watchers. It ...   \n",
       "4  This is a fabulous stew that came from one of ...   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  ['water', 'grits', 'salt', 'cheddar cheese', '...   \n",
       "1  ['onion', 'red bell pepper', 'garlic cloves', ...   \n",
       "2  ['white beans', 'canned black beans', 'tomatoe...   \n",
       "3  ['zucchini', 'yellow squash', 'diced tomatoes'...   \n",
       "4  ['beef stew meat', 'flour', 'salt', 'allspice'...   \n",
       "\n",
       "                                 ingredients_raw_str serving_size  servings  \\\n",
       "0  [\"4   cups    water\",\"1   cup   uncooked old f...    1 (155 g)         8   \n",
       "1  [\"1   medium    onion, chopped coarse \",\"1   m...    1 (366 g)         4   \n",
       "2  [\"1   cup   canned white beans, rinsed and dra...    1 (807 g)         1   \n",
       "3  [\"2       zucchini, sliced \",\"2   small    yel...    1 (244 g)         4   \n",
       "4  [\"3   lbs    beef stew meat\",\"3   tablespoons ...    1 (358 g)         8   \n",
       "\n",
       "                                               steps  \\\n",
       "0  ['I a sauce pan, bring water to a boil; slowly...   \n",
       "1  ['In a food processor, pulse the onion, red pe...   \n",
       "2  ['In a large bowl, combine beans, tomato, onio...   \n",
       "3  ['Put all ingredients in the crock pot and coo...   \n",
       "4  ['Preheat oven to 350Â°F.', \"Cut beef into 1 in...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['time-to-make', 'course', 'main-ingredient', ...   \n",
       "1  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "2  ['15-minutes-or-less', 'time-to-make', 'course...   \n",
       "3  ['weeknight', 'time-to-make', 'course', 'main-...   \n",
       "4  ['time-to-make', 'course', 'main-ingredient', ...   \n",
       "\n",
       "                                        search_terms  \n",
       "0  {'diabetic', 'low-calorie', 'vegetarian', 'low...  \n",
       "1                               {'dinner', 'shrimp'}  \n",
       "2  {'vegetarian', 'salad', 'side', 'dinner', 'veg...  \n",
       "3                  {'side', 'vegetarian', 'italian'}  \n",
       "4                                         {'dinner'}  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in dataset\n",
    "df = pd.read_csv(\n",
    "    \"recipes_w_search_terms.csv\", engine='python', encoding='utf-8'\n",
    ")\n",
    "\n",
    "# Clean unusual line terminators and extra spaces in all string columns\n",
    "df = df.map(\n",
    "    lambda x: str(x).replace('\\u2028', '\\n').replace('\\u2029', '\\n').strip()\n",
    "    if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Look at df information\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2OOFMv3qdwH"
   },
   "source": [
    "## 1.2 Data preprocessing and recipe text formatting\n",
    "This section prepares the raw recipe dataset for model training by cleaning and standardising the textual data. This is to ensure that all text in the data is consistent and can be understood by the model for modelling.\n",
    "\n",
    "Data preprocessing:\n",
    "- Clean strange line breaks and extra spaces across all text columns to avoid formatting errors and ensure that text is clean for model to learn from\n",
    "- Converts stringified lists into actual lists (\"['egg']\" --> ['egg']) such that it can be iterated through\n",
    "- Combine recipe parts (title, ingredients, and instructions) into structured and formatted text for easy reading and understanding\n",
    "- Stores all formatted recipes into list `dataset_stringified`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kKDpm09Ofl2y",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5eb5d7da-7a56-4c28-9c1e-d99e6e9aaf84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe #1\n",
      "----------------------------------------\n",
      "â­ï¸ Grilled Garlic Cheese Grits\n",
      "\n",
      "ğŸ¥¬\n",
      "\n",
      "â€¢ 4 cups water\n",
      "â€¢ 1 cup uncooked old fashion grits\n",
      "â€¢ 1 teaspoon salt\n",
      "â€¢ 4 ounces shredded cheddar cheese\n",
      "â€¢ 1 -2 clove garlic, minced\n",
      "â€¢ 1 tablespoon olive oil\n",
      "\n",
      "\n",
      "ğŸ¥£\n",
      "\n",
      "â–ªï¸ I a sauce pan, bring water to a boil; slowly add grits and salt, stirring constantly; Reduce heat:simmer, uncovered, for 40-45 minutes or untill thickened, stirrin occasionally.\n",
      "â–ªï¸ Add cheese and garlic; stir until cheese is melted, Spray 9-inch baking dish with nonstick cooking spray; Cover and refrigerate for 2 to 2 1/2 hours or until frim.\n",
      "â–ªï¸ Before starting the grill, coat the grill rack with nonstick cooking spray; Cut the grits into 3-inch squares; Brush both sides with olive oil.\n",
      "â–ªï¸ Grill, covered, over medium heat for 4 to 6 minutes on each side or until lightly browned.\n",
      "\n",
      "\n",
      "Recipe #2\n",
      "----------------------------------------\n",
      "â­ï¸ Simple Shrimp and Andouille Jambalaya\n",
      "\n",
      "ğŸ¥¬\n",
      "\n",
      "â€¢ 1 medium onion, chopped coarse\n",
      "â€¢ 1 medium red bell pepper, chopped coarse\n",
      "â€¢ 5 medium garlic cloves, chopped coarse\n",
      "â€¢ 1 lb extra large shrimp, shelled and deveined\n",
      "â€¢ salt\n",
      "â€¢ hot pepper sauce\n",
      "â€¢ 1 tablespoon vegetable oil\n",
      "â€¢ 3/4 lb andouille sausage, halved lengthwise and then cut into 1/4 inch slices\n",
      "â€¢ 1 1/2 cups long grain rice\n",
      "â€¢ 4 bay leaves\n",
      "â€¢ 1 (14 ounce) can diced tomatoes, briefly drained\n",
      "â€¢ 2 (8 ounce) bottles clam juice\n",
      "â€¢ 1/4 cup fresh parsley, chopped\n",
      "\n",
      "\n",
      "ğŸ¥£\n",
      "\n",
      "â–ªï¸ In a food processor, pulse the onion, red pepper and garlic until chopped very fine, ten to twelve 1-second pulses, scraping down the sides of the bowl as necessary; set aside.\n",
      "â–ªï¸ Season the shrimp with salt and hot pepper sauce to taste.\n",
      "â–ªï¸ Heat the oil in a large Dutch oven over medium high heat until shimmering. Add the shrimp in a single layer and cook, without stirring, for 30 seconds. Using tongs, flip the shrimp and cook for another 30 seconds. Transfer the shrimp to a medium bowl and set aside.\n",
      "â–ªï¸ Add the sausage to the pan and cook, stirring occasionally, until lightly browned, about 3 minutes. Using a slotted spoon, transfer the sausage to a second bowl.\n",
      "â–ªï¸ Scrape the veggies from the food processor into the empty pot and cook, stirring frequently, until softened, about 3 minutes. Stir in the rice and continue to stir until the grains are coated with the fat, about 1 minute.\n",
      "â–ªï¸ Add the bay leaves, tomatoes, clam juice, and 1 cup water and bring to a boil.\n",
      "â–ªï¸ Stir in the sausage, cover, and reduce the heat to low.\n",
      "â–ªï¸ Cook until the rice is tender, 17 to 20 minutes.\n",
      "â–ªï¸ Off the heat, stir in the shrimp and parsley with a fork (do not mush the rice), cover the pot, and allow to sit for 2 minutes or until the shrimp is heated through.\n",
      "â–ªï¸ Discard the bay leaves and adjust the seasoning with salt and hot pepper sauce to taste. Serve immediately.\n",
      "\n",
      "\n",
      "Recipe #3\n",
      "----------------------------------------\n",
      "â­ï¸ black-and-white bean salad\n",
      "\n",
      "ğŸ¥¬\n",
      "\n",
      "â€¢ 1 cup canned white beans, rinsed and drained\n",
      "â€¢ 1 cup canned black beans, rinsed and drained\n",
      "â€¢ 1 large tomatoes, diced\n",
      "â€¢ 1 small onion, diced\n",
      "â€¢ 1 stalk celery, diced\n",
      "â€¢ 1 tablespoon white wine vinegar\n",
      "â€¢ 2 tablespoons Italian parsley, minced\n",
      "â€¢ 1/8 teaspoon table salt, to taste\n",
      "â€¢ 1/8 teaspoon black pepper, to taste\n",
      "â€¢ 1 -2 teaspoon olive oil, to taste\n",
      "\n",
      "\n",
      "ğŸ¥£\n",
      "\n",
      "â–ªï¸ In a large bowl, combine beans, tomato, onion and celery.\n",
      "â–ªï¸ Gently stir in vinegar and sprinkle with parsley; season to taste and serve.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Section separators (for readability)\n",
    "stopword_title = \"â­ï¸ \"\n",
    "stopword_ingredient = \"\\nğŸ¥¬\\n\\n\"\n",
    "stopword_instruction = \"\\nğŸ¥£\\n\\n\"\n",
    "\n",
    "# Ensures that each ingredient is properly formatted to keep text consistent before concatenating them into our string\n",
    "def clean_ingredient_spaces(text):\n",
    "    if isinstance(text, str):\n",
    "        # Replace multiple spaces with one and trim leading/trailing spaces\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    elif isinstance(text, list):\n",
    "        # If it's a list of ingredients, clean each one\n",
    "        return [re.sub(r'\\s+', ' ', t).strip() for t in text if isinstance(t, str)]\n",
    "    return text\n",
    "\n",
    "# Converts stringified Python list into actual lists that can be iterated over\n",
    "def safe_extract_list(value):\n",
    "    \"\"\"Extract a list from stringified Python list or return empty list.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        # Handle patterns like \"['x', 'y']\" or [\"x\", \"y\"]\n",
    "        items = re.findall(r\"'([^']+)'\", value)\n",
    "        if not items:\n",
    "            items = re.findall(r'\"([^\"]+)\"', value)\n",
    "        return [v.strip() for v in items if v.strip()]\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        return list(value)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Converts recipe into structured text\n",
    "def recipe_to_string_simple(row):\n",
    "    \"\"\"Format recipe text using name, ingredients, and steps.\"\"\"\n",
    "    title = str(row.get(\"name\", \"\")).strip()\n",
    "    ingredients = safe_extract_list(row.get(\"ingredients_raw_str\"))\n",
    "    steps = safe_extract_list(row.get(\"steps\"))\n",
    "    # Format ingredient list\n",
    "    ingredients_string = \"\\n\".join(f\"â€¢ {ing}\" for ing in ingredients)\n",
    "    # Format step list\n",
    "    instructions_string = \"\\n\".join(f\"â–ªï¸ {step.strip()}\" for step in steps if step.strip())\n",
    "    # Combine everything\n",
    "    recipe_string = (\n",
    "        f\"{stopword_title}{title}\\n\"\n",
    "        f\"{stopword_ingredient}{ingredients_string}\\n\\n\"\n",
    "        f\"{stopword_instruction}{instructions_string.strip()}\"\n",
    "    )\n",
    "    return recipe_string\n",
    "\n",
    "# Apply functions to df\n",
    "df['ingredients_raw_str'] = df['ingredients_raw_str'].apply(clean_ingredient_spaces)\n",
    "\n",
    "# Clean every ingredient list\n",
    "dataset_stringified = []\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        dataset_stringified.append(recipe_to_string_simple(row))\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped recipe due to error: {e}\")\n",
    "\n",
    "# View first 3 recipes\n",
    "for i, recipe_text in enumerate(dataset_stringified[:3]):\n",
    "    print(f\"Recipe #{i+1}\\n{'-'*40}\")\n",
    "    print(recipe_text)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtJJkh2AtWvX"
   },
   "source": [
    "## 1.3 Filter and reduce size of recipe corpus\n",
    "Filter out recipes that are too short or too long, limiting the overall dataset size and improving data quality to make training more efficient.\n",
    "\n",
    "- Length of each recipe is estimated by splitting using the whitespaces\n",
    "- Recipes with length less than `min_len` or length more than `max_len` are removed from the dataset\n",
    "- Dataset is reduced to first **80,000 recipes**, after taking into the account computational resources we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oY06Dd6PfolH",
    "outputId": "c687bf8a-05e0-422d-87f4-22dc02acc2d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 494882 / 494963 recipes (length in [20,1000])\n"
     ]
    }
   ],
   "source": [
    "corpus = dataset_stringified # list[str]\n",
    "\n",
    "# Compute lengths (by token count approximation using whitespace)\n",
    "lengths = [len(s.split()) for s in corpus]\n",
    "min_len, max_len = 20, 1000\n",
    "\n",
    "# Filter indices\n",
    "keep_idx = [i for i, L in enumerate(lengths) if min_len <= L <= max_len]\n",
    "corpus_filtered = [corpus[i] for i in keep_idx]\n",
    "\n",
    "print(f\"Kept {len(corpus_filtered)} / {len(corpus)} recipes (length in [{min_len},{max_len}])\")\n",
    "\n",
    "# Reduce corpus (# of recipes) size\n",
    "corpus_filtered = corpus_filtered[:80000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAgu7jLEzXs1"
   },
   "source": [
    "## 1.4 Tokenisation\n",
    "This section converts `corpus_filtered` (recipe texts) into a numerical format such that the model can understand and learn. The model will learn from sequences of numbers that represent characters.\n",
    "\n",
    "- Initialise a character-level tokeniser for next-token prediction\n",
    "- Fit the tokeniser on `corpus_filtered` to build a complete character vocabulary and assign an index to each character\n",
    "- Converts all recipes to sequences of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6kHA-EEPfr36",
    "outputId": "b2aa79be-0405-4b5e-9904-48001cabcaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 151\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "stop_sign = '|' # custom stop sumbol for padding or truncation\n",
    "vocab_size = None # keep full vocab\n",
    "oov_token = \"<OOV>\" # marks any out-of-vocabulary characters\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True, num_words=vocab_size, oov_token=oov_token, filters='', lower=True)\n",
    "tokenizer.fit_on_texts([stop_sign])\n",
    "tokenizer.fit_on_texts(corpus_filtered)\n",
    "\n",
    "# Number of tokens (unique characters)\n",
    "print(\"Vocab size:\", len(tokenizer.word_index) + 1) # +1 for OOV token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KthvDxF0khj"
   },
   "source": [
    "## 1.5. Standardise length of recipes\n",
    "All the recipes are turned into a fixed-length sequence of numbers (either by padding or truncation) so that the model can efficiently learn to generate new recipes character by character.\n",
    "\n",
    "- **Padding:** makes all sequences the same length by adding extra dummy tokens (`'|'`) such that the model can easily identify non-content areas.\n",
    "- **Truncation:** makes all sequences the same length by shortening sequences that are too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGVCjxlFfxPu",
    "outputId": "763dd5d5-15e8-4cfc-baa9-d3d81db6977c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shape: (80000, 1001)\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(corpus_filtered)\n",
    "\n",
    "padded_without_stops = pad_sequences(sequences, maxlen=max_len-1, padding='post', truncating='post', value=tokenizer.texts_to_sequences([stop_sign])[0]) # 0 is the index of '|'\n",
    "padded = pad_sequences(padded_without_stops, maxlen=max_len+1, padding='post', truncating='post', value=tokenizer.texts_to_sequences([stop_sign])[0])\n",
    "\n",
    "print(\"Padded shape:\", padded.shape) # (N, maxlen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2rFzUPX3JoN"
   },
   "source": [
    "## 1.6 Helper functions\n",
    "- Function `sequence_to_recipe_string`: reverses the tokenisation process by converting tokenised recipe sequence (list or np.array of integers) back into a readable string version\n",
    "  - takes a tokenised recipe and removes padding tokens, if any\n",
    "  - converts token IDs back to characters\n",
    "  - joins tokens back into a single string such that it looks like a normal recipe again\n",
    "  - replaces special section tokens with readable emojis and labels for clarity\n",
    "- Function `to_lm_dataset`: transforms tokenised recipes into `(input, target)` pairs so that the model can learn next-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1hBqTegqf2ru",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1bb81c38-4feb-46a5-b3cc-af9688d5234b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ ï¸   g o l d e n   c h o c o l a t e   c h i p   m u f f i n s \n",
      " \n",
      " ğŸ¥¬ \n",
      " \n",
      " â€¢   1 / 2   c u p   b u t t e r \n",
      " â€¢   1   c u p   g r a n u l a t e d   s u g a r \n",
      " â€¢   2   t e a s p o o n s   b a k i n g   p o w d e r \n",
      " â€¢   1 / 2   t e a s p o o n   s a l t \n",
      " â€¢   1   t e a s p o o n   v a n i l l a   e x t r a c t \n",
      " â€¢   2   l a r g e   e g g s \n",
      " â€¢   1 / 2   c u p   m i l k \n",
      " â€¢   2   c u p s   w h o l e   w h e a t   f l o u r \n",
      " â€¢   2   c u p s   c h o c o l a t e   c h i p s \n",
      " â€¢   c o a r s e   d e c o r a t o r   s u g a r ,   f o r   t o p p i n g \n",
      " \n",
      " \n",
      " ğŸ¥£ \n",
      " \n",
      " â–ª ï¸   p r e h e a t   t h e   o v e n   t o   3 5 0 Â° f   l i g h t l y   g r e a s e   ( o r   l i n e   w i t h   m u f f i n   c u p s ,   a n d   g r e a s e   t h e   m u f f i n   c u p s )   a   s t a n d a r d - s i z e   m u f f i n   p a n . \n",
      " â–ª ï¸   b e a t   t o g e t h e r   t h e   b u t t e r ,   s u g a r ,   b a k i n g   p o w d e r ,   s a l t ,   a n d   v a n i l l a   u n t i l   f l u f f y .   b e a t   i n   t h e   e g g s   o n e   a t   a   t i m e ;   t h e n   s t i r   i n   t h e   m i l k .   m i x   i n   t h e   f l o u r ,   t h e n   t h e   c h o c o l a t e   c h i p s . \n",
      " â–ª ï¸   s p o o n   t h e   b a t t e r   i n t o   t h e   m u f f i n   c u p s ;   t h e y â€™ l l   b e   q u i t e   f u l l .   s p r i n k l e   e a c h   m u f f i n   w i t h   a   l i t t l e   c o a r s e   s u g a r .   b a k e   t h e   m u f f i n s   f o r   2 5   m i n u t e s ,   o r   u n t i l   a   c a k e   t e s t e r   i n s e r t e d   i n t o   t h e   c e n t e r   o f   o n e   c o m e s   o u t   w i t h o u t   c r u m b s   c l i n g i n g   t o   i t   ( a   l i t t l e   c h o c o l a t e   i s   o k ! ) . | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n"
     ]
    }
   ],
   "source": [
    "def sequence_to_recipe_string(sequence, tokenizer, remove_padding=True):\n",
    "    \"\"\"\n",
    "    Converts a tokenised recipe sequence (list or np.array of integers)\n",
    "    back into a readable string version.\n",
    "    \"\"\"\n",
    "    if isinstance(sequence, np.ndarray):\n",
    "        sequence = sequence.tolist()\n",
    "\n",
    "    if remove_padding:\n",
    "        sequence = [token for token in sequence if token != 0]\n",
    "\n",
    "    # Convert token IDs back to words\n",
    "    words = [tokenizer.index_word.get(token, \"<UNK>\") for token in sequence]\n",
    "\n",
    "    # Join into a single string\n",
    "    recipe_str = ' '.join(words)\n",
    "\n",
    "    # Format section tokens for readability\n",
    "    recipe_str = (recipe_str\n",
    "                  .replace(\"<TITLE>\", \"\\nâ­ï¸ \")\n",
    "                  .replace(\"<INGR>\", \"\\nğŸ¥¬\\n\")\n",
    "                  .replace(\"<INSTR>\", \"\\nğŸ¥£\\n\")\n",
    "                  .replace(\"<END>\", \"\\nâœ… End of recipe\\n\"))\n",
    "\n",
    "    return recipe_str.strip()\n",
    "\n",
    "# Example\n",
    "sample_sequence = padded[10]  # 10th recipe, tokenised\n",
    "recipe_text = sequence_to_recipe_string(sample_sequence, tokenizer)\n",
    "print(recipe_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOY05EMEf_oe"
   },
   "outputs": [],
   "source": [
    "def to_lm_dataset(padded_arrays):\n",
    "    # padded_arrays: np.ndarray shape (N, maxlen)\n",
    "    X = padded_arrays[:, :-1] # inputs: all tokens except the last one\n",
    "    y = padded_arrays[:, 1:] # targets: all tokens except the first one\n",
    "    # Wraps pairs (X, y) into a TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    return dataset\n",
    "\n",
    "lm_dataset = to_lm_dataset(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmAHwoDD5ASk"
   },
   "source": [
    "## 1.7 Split data into train and test\n",
    "Splits data into train/test on 90/10 split. Shuffles and batches them, and prepare data for efficient use during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWoVkt0UgBe2"
   },
   "outputs": [],
   "source": [
    "train_idx, test_idx = train_test_split(np.arange(len(padded)), test_size=0.1, random_state=42)\n",
    "\n",
    "train_arr = padded[train_idx]\n",
    "test_arr = padded[test_idx]\n",
    "train_ds = to_lm_dataset(train_arr).shuffle(10000).batch(64, drop_remainder=True).repeat()\n",
    "test_ds = to_lm_dataset(test_arr).shuffle(10000).batch(64, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PlOocoUAgEQl",
    "outputId": "a9114de6-c867-4637-d203-2a316ae93884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence size: 1000\n",
      "Target sequence size: 1000\n",
      "\n",
      "Input: 'â­ ï¸   t o f u   d r e a m   p u d d i n g   a n d   p i e   f i l l i n g \\n \\n ğŸ¥¬ \\n \\n â€¢   2 4   5 / 8'\n",
      "Target: 'ï¸   t o f u   d r e a m   p u d d i n g   a n d   p i e   f i l l i n g \\n \\n ğŸ¥¬ \\n \\n â€¢   2 4   5 / 8  '\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample batch (quick sanity check)\n",
    "for input_batch, target_batch in train_ds.take(1):\n",
    "    input_example = input_batch[0].numpy() # first sequence in batch\n",
    "    target_example = target_batch[0].numpy() # corresponding target sequence\n",
    "\n",
    "    print('Input sequence size:', len(input_example))\n",
    "    print('Target sequence size:', len(target_example))\n",
    "    print()\n",
    "\n",
    "    # Convert token IDs back to text using tokenizer\n",
    "    input_stringified = tokenizer.sequences_to_texts([input_example[:50]])[0]\n",
    "    target_stringified = tokenizer.sequences_to_texts([target_example[:50]])[0]\n",
    "\n",
    "    print('Input:', repr(''.join(input_stringified)))\n",
    "    print('Target:', repr(''.join(target_stringified)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YR-4d46fiS_"
   },
   "source": [
    "# 2. Model building and training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95G3qIpKLe5G"
   },
   "source": [
    "\n",
    "## 2.1 Build model\n",
    "Build a character-level Long Short-Term Memory (LSTM) language model that learns to predict the next character in a recipe, allowing it to generate entirely new recipes character by character.\n",
    "\n",
    "The model embeds the tokens in the embedding layer. The embedding layer transforms each numeric character ID into a 256-dimensional vector which helps the model learn relationships between characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6b1q_o0LeuuG"
   },
   "outputs": [],
   "source": [
    "# Define key parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1 # no. of unique characters\n",
    "embedding_dim = 256 # size of character embedding vector\n",
    "rnn_units = 512 # no. of hidden units in the LSTM layer\n",
    "BATCH_SIZE = 64 # no. of sequences processed together per training step\n",
    "\n",
    "batch_input_shape=[BATCH_SIZE, None]\n",
    "\n",
    "# Build model function\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(batch_shape=(batch_size, None)),\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "        tf.keras.layers.LSTM(\n",
    "            units=rnn_units,\n",
    "            return_sequences=True,\n",
    "            stateful=True,\n",
    "            recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
    "        ),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build model using function above\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmgYs2pqLCJm"
   },
   "source": [
    "Compile the model by specifying:\n",
    "- the loss function (`loss`)\n",
    "- the optimiser (`adam_optimizer`)\n",
    "- evaluation metrics (`'accuracy'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGWShFTfeu6r"
   },
   "outputs": [],
   "source": [
    "# Loss function (how errors are measured)\n",
    "def loss(labels, logits):\n",
    "  entropy = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "      y_true=labels, y_pred=logits,from_logits=True\n",
    "      )\n",
    "  return entropy\n",
    "\n",
    "# Optimiser (how the model updates its weights to reduce errors)\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=adam_optimizer,\n",
    "    loss=loss,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7-sPn4_MawI"
   },
   "source": [
    "## 2.2 Train model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxoNxbc1MABw"
   },
   "source": [
    "Prepare a folder to store model checkpoints (weights) during training, making it easier to save and reload training process in the future. Also saves weights for the fully trained model (epoch 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9md248P105uQ"
   },
   "outputs": [],
   "source": [
    "# To save weights at different epochs\n",
    "checkpoint_dir = 'tmp/checkpoints' # define a checkpoint directory\n",
    "os.makedirs(checkpoint_dir, exist_ok=True) # create the folder if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVsAmkcc076i"
   },
   "outputs": [],
   "source": [
    "# Configuring callbacks: stops the model early when any requirement is met\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    monitor='loss',\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}.weights.h5')\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3UtGtHiZdPV"
   },
   "source": [
    "To improve model accuracy and quality of output, we will run 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZYyvpZHneu9U",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "21382754-7dbd-4c23-eb0e-6598cb90d9ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 277ms/step - accuracy: 0.5946 - loss: 1.4779\n",
      "Epoch 2/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 278ms/step - accuracy: 0.7926 - loss: 0.6891\n",
      "Epoch 3/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 278ms/step - accuracy: 0.8133 - loss: 0.6114\n",
      "Epoch 4/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8239 - loss: 0.5734\n",
      "Epoch 5/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8302 - loss: 0.5507\n",
      "Epoch 6/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8345 - loss: 0.5354\n",
      "Epoch 7/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8376 - loss: 0.5240\n",
      "Epoch 8/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8399 - loss: 0.5159\n",
      "Epoch 9/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8418 - loss: 0.5093\n",
      "Epoch 10/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8433 - loss: 0.5039\n",
      "Epoch 11/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 280ms/step - accuracy: 0.8444 - loss: 0.5000\n",
      "Epoch 12/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8458 - loss: 0.4950\n",
      "Epoch 13/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8467 - loss: 0.4922\n",
      "Epoch 14/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8475 - loss: 0.4892\n",
      "Epoch 15/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 280ms/step - accuracy: 0.8483 - loss: 0.4860\n",
      "Epoch 16/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 280ms/step - accuracy: 0.8491 - loss: 0.4836\n",
      "Epoch 17/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8495 - loss: 0.4820\n",
      "Epoch 18/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8500 - loss: 0.4802\n",
      "Epoch 19/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8506 - loss: 0.4782\n",
      "Epoch 20/20\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 279ms/step - accuracy: 0.8511 - loss: 0.4763\n",
      "Restoring model weights from the end of the best epoch: 20.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_arr)//64,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        early_stopping_callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eSAm4kVgZb6",
    "outputId": "075f5052-18c8-4833-937d-846047fd6762"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model in Keras and h5 formats\n",
    "model.save(\"recipe_model.keras\")\n",
    "model.save(\"recipe_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6qauMDs1Zkt",
    "outputId": "c0a77c8c-e240-44a7-dbd0-c42d2cf29780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.8465 - loss: 0.4927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4873633086681366, 0.8482109904289246]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate trained model on test dataset\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejmpkh-NP1Lw"
   },
   "source": [
    "# 3. Load model\n",
    "Load previously trained model that was saved such that no retraining has to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TSwAC4CNKFP",
    "outputId": "d84b13a2-c2ed-41e8-f0e3-a1301c15027a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load previously trained model\n",
    "def loss(labels, logits):\n",
    "  entropy = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "      y_true=labels, y_pred=logits,from_logits=True\n",
    "      )\n",
    "  return entropy\n",
    "\n",
    "model = load_model(\"recipe_model.h5\", custom_objects={\"loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1L_E3Sv5YKLy"
   },
   "outputs": [],
   "source": [
    "# Create inference model (batch_size=1)\n",
    "inference_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "# Load trained weights\n",
    "inference_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz-BNJZT1Ya_"
   },
   "source": [
    "# 4. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8g6L5VYNAQb"
   },
   "source": [
    "## 4.1 Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXcTqeo28Z7R"
   },
   "outputs": [],
   "source": [
    "# Defining key metrics for evaluation\n",
    "\n",
    "# Perplexity\n",
    "def compute_perplexity(model, test_dataset, num_batches=100):\n",
    "    total_loss = 0\n",
    "    num_batches_processed = 0\n",
    "\n",
    "    for x_batch, y_batch in test_dataset.take(num_batches):\n",
    "        predictions = model(x_batch, training=False)\n",
    "        batch_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            y_batch, predictions, from_logits=True\n",
    "        )\n",
    "        total_loss += tf.reduce_mean(batch_loss).numpy()\n",
    "        num_batches_processed += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches_processed\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "# Repetition rate: how often model generates repetitive text\n",
    "def compute_repetition_rate(generated_samples, ngram_size=10):\n",
    "    repetition_count = 0\n",
    "\n",
    "    for text in generated_samples:\n",
    "        # Check if any ngram appears more than twice\n",
    "        for i in range(len(text) - ngram_size):\n",
    "            ngram = text[i:i+ngram_size]\n",
    "            if text[i+ngram_size:].count(ngram) >= 2:\n",
    "                repetition_count += 1\n",
    "                break\n",
    "\n",
    "    return repetition_count / len(generated_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_F8ZmaiNkkW"
   },
   "source": [
    "## 4.2 Generate samples for qualitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvLYqppTgZO6",
    "outputId": "e976f947-3a7b-4f7c-f2b9-6351b7502cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredient: \"potato\"\n",
      "----------------------------------------\n",
      "potato cates apple flavoured like wine strips)\n",
      "â€¢ 450 g pears or 100 g brown turkey prepared cherries\n",
      "â€¢ 150 ml scraped pizza crust\n",
      "â€¢ 8 eggs\n",
      "â€¢ 225 g ricotta cheese\n",
      "â€¢ 3 ices mayonnaise\n",
      "â€¢ 1/4 cup cheddar cheese, shredded\n",
      "\n",
      "\n",
      "ğŸ¥£\n",
      "\n",
      "â–ªï¸ preheat oven to 300Â°f degrees.\n",
      "â–ªï¸ preheat oven to 350 degrees.\n",
      "â–ªï¸ in a large skillet, cook the croves and bay leaf noodles and oil or bowl.\n",
      "â–ªï¸ add honey and cinnamon, all softer, except zucchini and cook an oven-edge of pan just until the, 15co time, for two minutes.\n",
      "â–ªï¸ stir in cho\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, num_generate=1000):\n",
    "    # Convert start string to token IDs\n",
    "    input_indices = np.array(tokenizer.texts_to_sequences([start_string]))\n",
    "\n",
    "    input_indices = tf.convert_to_tensor(input_indices, dtype=tf.int32)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # Reset LSTM states\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'reset_states'):\n",
    "            layer.reset_states()\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_indices)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Sample from categorical distribution\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Update input_indices for next step\n",
    "        input_indices = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        # Convert token ID back to word\n",
    "        next_word = tokenizer.sequences_to_texts(input_indices.numpy())[0]\n",
    "        text_generated.append(next_word)\n",
    "\n",
    "    return start_string + ' ' + ''.join(text_generated)\n",
    "\n",
    "def generate_combinations(model):\n",
    "    recipe_length = 500  # adjust length as desired\n",
    "    ingredients = ['potato']\n",
    "\n",
    "    for ingredient in ingredients:\n",
    "        print(f'Ingredient: \"{ingredient}\"')\n",
    "        print('-'*40)\n",
    "        generated_text = generate_text(model, start_string=ingredient, num_generate=recipe_length)\n",
    "        print(generated_text)\n",
    "        print('\\n\\n')\n",
    "\n",
    "# Run generation\n",
    "generate_combinations(model_simplified)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjBXnP6DO-0i"
   },
   "source": [
    "## 4.3 Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Mt0xKFy8byL"
   },
   "outputs": [],
   "source": [
    "def generate_samples(model, tokenizer, n_samples=20, num_generate=800, batch_size=1):\n",
    "    \"\"\"Generate recipe samples for evaluation\"\"\"\n",
    "    # Build a generation model with batch_size=1\n",
    "    gen_model = tf.keras.Sequential([\n",
    "        tf.keras.Input(batch_shape=(1, None)),\n",
    "        tf.keras.layers.Embedding(input_dim=model.layers[0].input_dim, output_dim=model.layers[0].output_dim,\n",
    "                                   weights=model.layers[0].get_weights()),\n",
    "        tf.keras.layers.LSTM(units=model.layers[1].units, return_sequences=True, stateful=True,\n",
    "                            recurrent_initializer=tf.keras.initializers.GlorotNormal()),\n",
    "        tf.keras.layers.Dense(model.layers[2].units)\n",
    "    ])\n",
    "\n",
    "    # Copy weights from trained model\n",
    "    gen_model.layers[1].set_weights(model.layers[1].get_weights())\n",
    "    gen_model.layers[2].set_weights(model.layers[2].get_weights())\n",
    "\n",
    "    prompts = [\n",
    "        \"â­ï¸ chocolate chip cookies\",\n",
    "        \"â­ï¸ pasta\",\n",
    "        \"â­ï¸ chicken\",\n",
    "        \"â­ï¸ salad\",\n",
    "        \"â­ï¸ soup\"\n",
    "    ]\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        prompt = prompts[i % len(prompts)]\n",
    "\n",
    "        input_indices = np.array(tokenizer.texts_to_sequences([prompt]))\n",
    "        input_indices = tf.convert_to_tensor(input_indices, dtype=tf.int32)\n",
    "\n",
    "        text_generated = []\n",
    "\n",
    "        # Reset LSTM states\n",
    "        for layer in gen_model.layers:\n",
    "            if hasattr(layer, 'reset_states'):\n",
    "                layer.reset_states()\n",
    "\n",
    "        for _ in range(num_generate):\n",
    "            predictions = gen_model(input_indices)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "            input_indices = tf.expand_dims([predicted_id], 0)\n",
    "            next_word = tokenizer.sequences_to_texts(input_indices.numpy())[0]\n",
    "            text_generated.append(next_word)\n",
    "\n",
    "        samples.append(prompt + ''.join(text_generated))\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Perplexity (model confidence)\n",
    "perplexity = compute_perplexity(model, test_ds, num_batches=100)\n",
    "print(f\"Perplexity:{perplexity:.2f}\")\n",
    "\n",
    "# Generate samples\n",
    "samples = generate_samples(model, tokenizer, n_samples=20)\n",
    "\n",
    "# Repetition rate\n",
    "repetition_rate = compute_repetition_rate(samples, ngram_size=10)\n",
    "print(f\"Repetition rate:{repetition_rate:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
