{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxCcWsq7fr7K"
   },
   "source": [
    "# 1. Data preparation\n",
    "In this section, the data will first be prepared for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ5XZJUo8-rY"
   },
   "source": [
    "Install required external Python libraries for training and evaluation of the Flan-T5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install evaluate\n",
    "%pip install -U transformers\n",
    "%pip install rouge_score\n",
    "%pip install bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0Hw2j_39fOO"
   },
   "source": [
    "## 1.1 Load necessary libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "# Set memory optimization environment variables\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/recipe_model/checkpoints_test\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import torch\n",
    "import evaluate\n",
    "import ast\n",
    "import time\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq, T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from google.colab import drive\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\n",
    "from tqdm import tqdm\n",
    "from bert_score import score as bert_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # clear GPU memory\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"recipes_w_search_terms.csv\",\n",
    "    engine='python',\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# Clean unusual line terminators and strip spaces\n",
    "df = df.map(\n",
    "    lambda x: str(x).replace('\\u2028', '\\n').replace('\\u2029', '\\n').strip().lower()\n",
    "    if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rpuKV75_CRB"
   },
   "source": [
    "## 1.2 Data cleaning and preprocessing\n",
    "The raw dataset has to be cleaned and reformatted into a structure suitable for modelling and text-to-text generation.\n",
    "\n",
    "Many of the fields in this dataset are stored as string representations of Python lists, hence it needs to be safely parsed into actual lists. After parsing, the data is normalised by converting all text to lowercase and removing unnecessary whitespace. This ensures that the format is consistent and does not introduce noise into the training process. Rows that lack important information like the `ingredients` and `steps` are removed.\n",
    "\n",
    "Each recipe is transformed into an input-output pair that aligns with the model's objective. The model's input is formed by joining the cleaned ingredient names into a single comma-separated string, while the target output will be a structured text block consisting of the recipe title, the expanded ingredient list, and the instructions. These are stored in the `input_text` and `target_text` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safely parse list-like strings\n",
    "def safe_parse(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# Normalise whitespace and lowercase\n",
    "def clean_whitespace(x):\n",
    "    if isinstance(x, str):\n",
    "        return re.sub(r'\\s+', ' ', x).strip().lower()\n",
    "    elif isinstance(x, list):\n",
    "        return [re.sub(r'\\s+', ' ', i).strip().lower() for i in x if isinstance(i, str)]\n",
    "    return x\n",
    "\n",
    "# Apply cleaning functions\n",
    "df[\"ingredients\"] = df[\"ingredients\"].apply(safe_parse).apply(clean_whitespace)\n",
    "df[\"ingredients_raw_str\"] = df[\"ingredients_raw_str\"].apply(safe_parse).apply(clean_whitespace)\n",
    "df[\"steps\"] = df[\"steps\"].apply(safe_parse).apply(clean_whitespace)\n",
    "\n",
    "# Drop incomplete rows\n",
    "df = df[df[\"ingredients\"].map(len) > 0]\n",
    "df = df[df[\"steps\"].map(len) > 0]\n",
    "\n",
    "# Format examples for training\n",
    "def format_training_example(row):\n",
    "    title = str(row.get(\"name\", \"\")).strip().lower()\n",
    "    ingredients_in = \", \".join(row[\"ingredients\"])\n",
    "    ingredients_out = \"\\n\".join(row[\"ingredients_raw_str\"])\n",
    "    steps = \"\\n\".join(row[\"steps\"])\n",
    "    recipe_text = f\"{title}\\n\\ningredients:\\n{ingredients_out}\\n\\nsteps:\\n{steps}\"\n",
    "    return ingredients_in, recipe_text\n",
    "\n",
    "# Apply formatting\n",
    "df[\"input_text\"], df[\"target_text\"] = zip(*df.apply(format_training_example, axis=1))\n",
    "\n",
    "# Drop rows missing either field and reset index\n",
    "df = df.dropna(subset=[\"input_text\", \"target_text\"]).reset_index(drop=True)\n",
    "\n",
    "# Get the validation set\n",
    "sampling_df = df[80001:]\n",
    "df = df[:80000]\n",
    "\n",
    "# Show example\n",
    "print(df.head(1)[[\"input_text\", \"target_text\"]].to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INNA0eyCFrmg"
   },
   "source": [
    "## 1.3 Split data\n",
    "After cleaning and formatting the dataset, it is first split into train and test sets using a 90/10 split, with a fixed random seed to ensure reproducibility.\n",
    "\n",
    "As we are training a Hugging Face transformer model, we need a validation set as well. However, we did not include that in our LSTM model training. As such, we created a separate validation set by sampling from the previously reserved `sampling_df`, providing an independent subset for tuning hyperparameters and monitoring model performance during training. \\\n",
    "This results in a train, validation, and test datasets with a split of 80/10/10.\n",
    "\n",
    "Each of these subsets is then converted into Hugging Face `Dataset` objects, allowing efficient integration with the `Seq2SeqTrainer` and other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hugging face dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "val_df = sampling_df.sample(n=8000, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YR-4d46fiS_"
   },
   "source": [
    "# 2. Model building and training (FLAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N78LjTE4G0gg"
   },
   "source": [
    "## 2.1 Tokenisation and input preparation\n",
    "Different from the steps taken when training the LSTM model, the Flan-T5 transformer model is tokenised after splitting and before model training. This is due to the two models using different training paradigms.\n",
    "- LSTM requires the data to be tokenised before splitting as the data structure (X, Y) depends on it.\n",
    "- Flan-T5 tokenisation does no depend on X/Y shifting, so it happens after splitting of dataset to prevent data leakage.\n",
    "- X = tokens[0:n-1], Y = tokens[1:n]\n",
    "\n",
    "After splitting the dataset into training, validation, and test datasets, the raw text fields are converted into token IDs that the Flan-T5 model can process.\n",
    "\n",
    "The pretrained Flan-T5 tokeniser and model are loaded, and the maximum sequence lengths for both the input and the target output are defined. The ingredient list is prefixed with an instruction prompt so that the model understands the task and is consistent with its generation behaviour. Both inputs and target recipe texts are tokenised using fixed maximum lengths, with truncation and padding applied to maintain uniform tensor shapes.\n",
    "\n",
    "Padding tokens are replaced with `-100` so that they are ignored during loss computation (standard practice for seq2seq models). The preprocessing function is applied to all datasets, producing tokenised datasets that are ready for training.\n",
    "\n",
    "`DataCollatorForSeq2Seq` dynamically batches and pad examples during training in a way that is compatible with Flan-T5's encoder-decoder architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"google/flan-t5-base\"\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "max_input = 128\n",
    "max_target = 256\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"Generate a recipe with measurements using: \" + ing for ing in examples[\"input_text\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Tokenize targets (labels)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target_text\"],\n",
    "            max_length=max_target,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCUgayU4JgcW"
   },
   "source": [
    "## 2.2 LoRA configuration (parameter-efficient fine-tuning (PEFT))\n",
    "Low-Rank Adaptation (LoRA) is applied to fine-tune and train Flan-T5 efficiently. This allows the model to adapt to the recipe-generation task while keeping most of the pretrained weights frozen, resulting in significantly lower memory usage and faster training.\n",
    "\n",
    "LoRA hyperparameters are defined to balance training stability and model quality. Target modules corresponding to the key projection layers within the T5 attention blocks where LoRA adaptors should be injected are also specified.\n",
    "\n",
    "The base model is then wrapped with the LoRA adaptors using `get_peft_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\", \"k\", \"o\"], # for more comprehensive targeting\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Wrap base model with LoRA adaptors\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Verify that PEFT is correctly applied\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDzWRP60LlIH"
   },
   "source": [
    "## 2.3 Training configuration and hyperparameters\n",
    "The training hyperparameters are defined in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=2000,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=2000,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    fp16=False,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    report_to=\"none\",\n",
    "    max_grad_norm=1.0,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    predict_with_generate=True,\n",
    "    dataloader_num_workers=1,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    dataloader_pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory status\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory free: {torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8d-MPyDQGb4"
   },
   "source": [
    "## 2.4 Initialise the Trainer\n",
    "This section initialises a Hugging Face `Seq2SeqTrainer`, which provides a high-level training loop tailored for encoder-decoder models like Flan-T5.\n",
    "\n",
    "It handles batching, forward and backward passes, gradient updates, evaluation, and checkpointing, simplifying the fine-tuning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyORwCX8So0o"
   },
   "source": [
    "## 2.5 Train model\n",
    "The model is trained in this section. During training, the model iteratively updates its LoRA adaptor weigfhts based on the training data while periodically evaluating its performance on the validation set.\n",
    "\n",
    "The trained model is saved to Google Drive such that it can be reloaded later for evaluation without retraining. The tokeniser is also saved to ensure that preprocessing and text generation remain consistent across current and future sessions.\n",
    "\n",
    "Model was trained in separate sessions as T4 GPU time ran out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save model to drive\n",
    "save_path = \"/content/drive/MyDrive/recipe_model/final_model\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Path model is saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from previous saved checkpoint\n",
    "checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint\")]\n",
    "print(OUTPUT_DIR)\n",
    "latest_checkpoint = max(\n",
    "    checkpoints,\n",
    "    key=lambda x: int(re.search(r\"checkpoint-(\\d+)\", x).group(1))\n",
    ")\n",
    "\n",
    "latest_checkpoint = os.path.join(OUTPUT_DIR, latest_checkpoint)\n",
    "print(f\"Resuming from: {latest_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training\n",
    "trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "\n",
    "# Save model to drive\n",
    "save_path = \"/content/drive/MyDrive/recipe_model/final_model\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Path model is saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDSjaFQAeknL"
   },
   "source": [
    "# 3. Model evaluation (FLAN-T5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTcHG_NOKjwj"
   },
   "source": [
    "## 3.1 Reloading model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/content/drive/MyDrive/recipe_model/final_model\"\n",
    "\n",
    "# Load the base model\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/flan-t5-small\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "\n",
    "# Merge LoRA weights for faster inference\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Move to GPU and set to eval mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZEKefLZKwEL"
   },
   "source": [
    "## 3.2 Generating sample recipes for qualitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate recipe from list of ingredients\n",
    "def generate_recipe(ingredients, model, tokenizer, max_length=256):\n",
    "    # Join ingredients\n",
    "    if isinstance(ingredients, list):\n",
    "        ingredients_text = \", \".join(ingredients)\n",
    "    else:\n",
    "        ingredients_text = ingredients\n",
    "\n",
    "    input_text = f\"Generate a recipe with measurements using: {ingredients_text}\"\n",
    "\n",
    "    # Tokenise\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Move inputs to same device as model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test\n",
    "ingredients = [\"chicken breast\", \"garlic\", \"olive oil\", \"lemon\", \"pepper\"]\n",
    "recipe = generate_recipe(ingredients, model, tokenizer)\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing various ingredients\n",
    "# Example 1\n",
    "ingredients = [\"onion\", \"salt\", \"flour\"]\n",
    "recipe1 = generate_recipe(ingredients, model, tokenizer)\n",
    "print(recipe1)\n",
    "\n",
    "# Example 2\n",
    "ingredients = [\"chocolate\", \"salt\"]\n",
    "recipe2 = generate_recipe(ingredients, model, tokenizer)\n",
    "print(recipe2)\n",
    "\n",
    "# Example 3\n",
    "ingredients = [\"potato\", \"flour\", \"onion\", \"garlic\", \"oil\"]\n",
    "recipe3 = generate_recipe(ingredients, model, tokenizer)\n",
    "print(recipe3)\n",
    "\n",
    "# Example 4\n",
    "ingredients = [\"egg\", \"leek\"]\n",
    "recipe4 = generate_recipe(ingredients, model, tokenizer)\n",
    "print(recipe4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIUsvJREK_WH"
   },
   "source": [
    "## 3.3 Stratified sampling & batch generation\n",
    "Instead of running inference on the full test set, a smaller representative sample is selected to reduce computation time. Recipe generation is then performed in batches, which allows the model to process multiple inputs simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 500  # reduce from 8000 to 500\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(test_dataset), size=min(SAMPLE_SIZE, len(test_dataset)), replace=False)\n",
    "\n",
    "# Batch generation for speed\n",
    "def generate_batch(input_texts, model, tokenizer, batch_size=8, max_length=256):\n",
    "    all_predictions = []\n",
    "    num_batches = (len(input_texts) + batch_size - 1) // batch_size\n",
    "\n",
    "    with tqdm(total=len(input_texts), desc=\"Generating recipes\", unit=\"recipe\") as pbar:\n",
    "        for i in range(0, len(input_texts), batch_size):\n",
    "            batch = input_texts[i:i+batch_size]\n",
    "            inputs = [\"Generate a recipe with measurements using: \" + ing for ing in batch]\n",
    "\n",
    "            # Tokenise batch\n",
    "            model_inputs = tokenizer(\n",
    "                inputs,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=128,\n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            )\n",
    "            model_inputs = {k: v.to(model.device) for k, v in model_inputs.items()}\n",
    "\n",
    "            # Generate for batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **model_inputs,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    temperature=0.7,\n",
    "                )\n",
    "\n",
    "            # Decode batch\n",
    "            batch_predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            all_predictions.extend(batch_predictions)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "            # Show speed estimate every 50 recipes\n",
    "            if len(all_predictions) % 50 == 0:\n",
    "                pbar.set_postfix({\"completed\": f\"{len(all_predictions)}/{len(input_texts)}\"})\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "# Generate predictions for sampled test set\n",
    "print(\"Generating predictions with batching...\")\n",
    "input_texts = [test_dataset[int(i)][\"input_text\"] for i in sample_indices]\n",
    "references = [test_dataset[int(i)][\"target_text\"] for i in sample_indices]\n",
    "\n",
    "# Batch generation\n",
    "predictions = generate_batch(input_texts, model, tokenizer, batch_size=8)\n",
    "print(f\"Generated {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7zaanPoRF-k"
   },
   "source": [
    "## 3.4 Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTScore (semantic similarity)\n",
    "P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=True)\n",
    "print(f\"  Precision: {P.mean():.4f}\")\n",
    "print(f\"  Recall: {R.mean():.4f}\")\n",
    "print(f\"  F1: {F1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract ingredients from recipe text\n",
    "def extract_ingredients(text):\n",
    "    if \"ingredients:\" in text.lower():\n",
    "        parts = text.lower().split(\"ingredients:\")\n",
    "        if len(parts) > 1:\n",
    "            ing_section = parts[1].split(\"steps:\")[0] if \"steps:\" in parts[1] else parts[1]\n",
    "            return [line.strip() for line in ing_section.strip().split('\\n') if line.strip()]\n",
    "    return []\n",
    "\n",
    "# Defining function to calculate ingredient coverage: % of true ingredients that appear in predictions\n",
    "def ingredient_coverage(pred_ingredients, true_ingredients):\n",
    "    if not true_ingredients:\n",
    "        return 0.0\n",
    "    pred_text = \" \".join(pred_ingredients).lower()\n",
    "    matches = sum(1 for ing in true_ingredients if any(word in pred_text for word in ing.lower().split()))\n",
    "    return matches / len(true_ingredients)\n",
    "\n",
    "# Compile average ingredient coverage across generated recipes\n",
    "ingredient_coverages = []\n",
    "\n",
    "for pred, ref in zip(predictions, references):\n",
    "    pred_ings = extract_ingredients(pred)\n",
    "    ref_ings = extract_ingredients(ref)\n",
    "\n",
    "    coverage = ingredient_coverage(pred_ings, ref_ings)\n",
    "    ingredient_coverages.append(coverage)\n",
    "\n",
    "print(f\"\\n  Avg Ingredient Coverage: {np.mean(ingredient_coverages):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating sample vs true recipes for human evaluation\n",
    "for i in range(min(30, len(predictions))):\n",
    "    print(f\"\\nðŸ”¹ Example {i+1}\")\n",
    "    print(f\"Input: {input_texts[i]}\")\n",
    "    print(f\"\\nGenerated Recipe:\\n{predictions[i]}\")\n",
    "    print(f\"\\nTrue Recipe:\\n{references[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pYWpPsRqzsC"
   },
   "source": [
    "# 4. Ablation Studies (FLAN-T5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size for ablation\n",
    "ABLATION_SAMPLE_SIZE = 100\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(test_dataset), size=ABLATION_SAMPLE_SIZE, replace=False)\n",
    "\n",
    "# Define different prompt formats\n",
    "prompt_formats = {\n",
    "    \"format_0\": \"Generate a recipe with measurements using: {ingredients}\" # original\n",
    "    \"format_1\": \"Create a recipe with ingredient measurements using: {ingredients}\", # synonym + measurement emphasis\n",
    "    \"format_2\": \"Given the ingredients {ingredients}, write a complete recipe including quantities and steps.\", # explicit structure\n",
    "    \"format_3\": \"You are a chef. Develop a detailed recipe with exact measurements for: {ingredients}\", # persona + precision\n",
    "    \"format_4\": \"From these ingredients: {ingredients}, plan and write a step-by-step recipe including measurements.\" # reasoning cue + step structure\n",
    "}\n",
    "\n",
    "# Generate recipes using specific prompt format\n",
    "def generate_with_prompt(ingredients, prompt_template, model, tokenizer):\n",
    "    # Format the prompt\n",
    "    prompt = prompt_template.format(ingredients=ingredients)\n",
    "\n",
    "    # Tokenise\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Move inputs to same device as model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=256,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt_results = {}\n",
    "\n",
    "for format_name, prompt_template in prompt_formats.items():\n",
    "    print(f\"\\nTesting {format_name}: '{prompt_template}'\")\n",
    "\n",
    "    generations = []\n",
    "    references = []\n",
    "    coverages = []\n",
    "\n",
    "    for idx in tqdm(sample_indices, desc=f\"Generating with {format_name}\"):\n",
    "        input_text = test_dataset[int(idx)]['input_text']\n",
    "        reference = test_dataset[int(idx)]['target_text']\n",
    "\n",
    "        # Extract just ingredients\n",
    "        if \"using:\" in input_text.lower():\n",
    "            ingredients = input_text.split(\"using:\")[-1].strip()\n",
    "        else:\n",
    "            ingredients = input_text\n",
    "\n",
    "        # Generate\n",
    "        generated = generate_with_prompt(ingredients, prompt_template, model, tokenizer)\n",
    "\n",
    "        generations.append(generated)\n",
    "        references.append(reference)\n",
    "\n",
    "        # Calculate metrics\n",
    "        coverage = ingredient_coverage(generated, ingredients)\n",
    "        coverages.append(coverage)\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    print(f\"  Computing BERTScore...\")\n",
    "    P, R, F1 = bert_score(generations, references, lang=\"en\", verbose=False, batch_size=32)\n",
    "\n",
    "    prompt_results[format_name] = {\n",
    "        'prompt': prompt_template,\n",
    "        'bertscore_f1': F1.mean().item(),\n",
    "        'coverage': np.mean(coverages)\n",
    "    }\n",
    "\n",
    "    print(f\"Results: BERTScore F1={F1.mean():.4f}, Ingredient Coverage={np.mean(coverages):.2%}\")\n",
    "\n",
    "# Create comparison table\n",
    "ablation_df = pd.DataFrame(prompt_results).T\n",
    "ablation_df = ablation_df.round(4)\n",
    "print(ablation_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5Tu9Pt15V5Y"
   },
   "source": [
    "# 5. Model building (Retriever)\n",
    "Use fine-tuned t5 model with the adaptor to use it as a feature extractor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your saved adapter folder\n",
    "model_dir = \"/content/drive/MyDrive/recipe_model/final_model\"\n",
    "\n",
    "# Load adapter config\n",
    "config = PeftConfig.from_pretrained(model_dir)\n",
    "\n",
    "# Load base model\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the fine-tuned LoRA adapter weights\n",
    "model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch embedding function\n",
    "def get_t5_embeddings(texts, model, tokenizer, batch_size=16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    all_embeddings = []\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = model.base_model.encoder(**inputs)\n",
    "            # Mean pool across tokens\n",
    "            batch_embeddings = encoder_outputs.last_hidden_state.mean(dim=1)\n",
    "            all_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['ingredients'].astype(str).tolist()\n",
    "\n",
    "# Generate and save\n",
    "recipe_embeddings = get_t5_embeddings(texts, model, tokenizer, batch_size=16)\n",
    "np.save(\"/content/drive/MyDrive/recipe_model/final_model/recipe_embeddings\", recipe_embeddings)\n",
    "\n",
    "print(\"Saved recipe embeddings:\", recipe_embeddings.shape)\n",
    "print(\"Saved recipe embeddings to:\", model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings (without training the code chunk directly above this chunk)\n",
    "recipe_embeddings = np.load(\"/content/drive/MyDrive/recipe_model/final_model/recipe_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval function\n",
    "def recommend_recipes(query, model, tokenizer, recipe_embeddings, recipe_texts, top_k=5):\n",
    "    query_emb = get_t5_embeddings(query, model, tokenizer)\n",
    "    sims = cosine_similarity(query_emb, recipe_embeddings)[0]\n",
    "    top_idx = np.argsort(sims)[::-1][:top_k]\n",
    "    results = [(recipe_texts[i], sims[i]) for i in top_idx]\n",
    "    return results\n",
    "\n",
    "texts = df['ingredients'].astype(str).tolist()\n",
    "\n",
    "query = \"chicken, rice, soy sauce\"\n",
    "recommendations = recommend_recipes(query, model, tokenizer, recipe_embeddings, texts)\n",
    "\n",
    "for recipe, score in recommendations:\n",
    "    print(f\"{score:.3f} â€“ {recipe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve top 5 similar recipes to the query ingredients\n",
    "top_recipes = recommend_recipes(query, model, tokenizer, recipe_embeddings, df['ingredients'].tolist(), top_k=5)\n",
    "\n",
    "# Build a prompt from retrieved recipes\n",
    "query_emb = get_t5_embeddings(query, model, tokenizer)\n",
    "sims = cosine_similarity(query_emb, recipe_embeddings)[0]\n",
    "top_idx = np.argsort(sims)[::-1][:5] # numeric indices\n",
    "\n",
    "context = \"\\n\\n\".join(\n",
    "    \" \".join(df['steps'].iloc[idx]) if isinstance(df['steps'].iloc[idx], list)\n",
    "    else str(df['steps'].iloc[idx])\n",
    "    for idx in top_idx\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Available ingredients: {query}.\n",
    "\n",
    "Reference recipes:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnSItfVWTMEq"
   },
   "source": [
    "# 6. Model evaluation (Retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgIV5SwnTapg"
   },
   "source": [
    "## 6.1 Stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original dataframe indices for test set\n",
    "test_df_indices = test_df.index.tolist()\n",
    "\n",
    "# Sample from test set for faster evaluation\n",
    "np.random.seed(42)\n",
    "SAMPLE_SIZE = 500\n",
    "sample_size = min(SAMPLE_SIZE, len(test_dataset))\n",
    "\n",
    "# Create test queries using existing test set\n",
    "test_queries = []\n",
    "for i in range(sample_size):\n",
    "    original_idx = test_df_indices[i] # map to original df index\n",
    "\n",
    "    test_queries.append({\n",
    "        'test_dataset_idx': i, # index in test_dataset\n",
    "        'original_df_idx': original_idx, # index in original df\n",
    "        'query_ingredients': test_dataset[i]['input_text'],\n",
    "        'true_recipe': test_dataset[i]['target_text'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LI1Kml8MTV29"
   },
   "source": [
    "## 6.2 Key metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-1 similarity\n",
    "top1_similarities = []\n",
    "retrieval_times = []\n",
    "all_retrievals = []\n",
    "\n",
    "for test_item in tqdm(test_queries, desc=\"Testing retrieval\"):\n",
    "    query = test_item['query_ingredients']\n",
    "    query_idx = test_item['original_df_idx'] # use original df index\n",
    "\n",
    "    # Time the retrieval\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Get query embedding and similarities\n",
    "    query_emb = get_t5_embeddings([query], model, tokenizer)\n",
    "    sims = cosine_similarity(query_emb, recipe_embeddings)[0]\n",
    "\n",
    "    # Get top-5 (excluding query itself)\n",
    "    all_indices = np.argsort(sims)[::-1]\n",
    "    top_idx = [i for i in all_indices if i != query_idx][:5]\n",
    "\n",
    "    retrieval_time = time.time() - start_time\n",
    "\n",
    "    # Store metrics\n",
    "    top1_similarities.append(sims[top_idx[0]])\n",
    "    retrieval_times.append(retrieval_time)\n",
    "\n",
    "    # Store for later use\n",
    "    all_retrievals.append({\n",
    "        'query': query,\n",
    "        'query_idx': query_idx,\n",
    "        'top1_idx': top_idx[0],\n",
    "        'top1_similarity': sims[top_idx[0]],\n",
    "        'top5_indices': top_idx,\n",
    "        'top5_similarities': [sims[i] for i in top_idx],\n",
    "        'true_recipe': test_item['true_recipe']\n",
    "    })\n",
    "\n",
    "# Results\n",
    "avg_top1_sim = np.mean(top1_similarities)\n",
    "\n",
    "print(f\"Average Top-1 Similarity:{avg_top1_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision@5\n",
    "precision_scores = []\n",
    "\n",
    "# Define \"relevant\" as similarity > 0.7\n",
    "RELEVANCE_THRESHOLD = 0.7\n",
    "\n",
    "for retrieval in tqdm(all_retrievals, desc=\"Calculating precision\"):\n",
    "    print(retrieval)\n",
    "    top5_sims = retrieval['top5_similarities']\n",
    "\n",
    "    # Count how many of top-5 are relevant\n",
    "    relevant_count = sum(1 for sim in top5_sims if sim >= RELEVANCE_THRESHOLD)\n",
    "\n",
    "    precision = relevant_count / 5\n",
    "    precision_scores.append(precision)\n",
    "\n",
    "avg_precision = np.mean(precision_scores)\n",
    "std_precision = np.std(precision_scores)\n",
    "\n",
    "print(f\"Average Precision@5:  {avg_precision:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKOpRT0ZrbXE"
   },
   "source": [
    "# 7. Ablation Studies (Retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine vs Euclidean vs Manhattan\n",
    "np.random.seed(42)\n",
    "\n",
    "RETRIEVAL_SAMPLE_SIZE = 500\n",
    "K_VALUES = [1, 3, 5, 7, 10]\n",
    "RELEVANCE_THRESHOLD = 0.7\n",
    "\n",
    "# Sample test queries and convert indices\n",
    "retrieval_sample_indices = np.random.choice(\n",
    "    len(test_dataset),\n",
    "    size=min(RETRIEVAL_SAMPLE_SIZE, len(test_dataset)),\n",
    "    replace=False\n",
    ")\n",
    "retrieval_sample_indices = [int(idx) for idx in retrieval_sample_indices]\n",
    "\n",
    "# Define similarity metrics\n",
    "similarity_methods = {\n",
    "    \"cosine\": lambda q, E: cosine_similarity(q, E)[0],\n",
    "    \"euclidean\": lambda q, E: 1 - (pairwise_distances(q, E, metric='euclidean')[0] / pairwise_distances(q, E, metric='euclidean')[0].max()),\n",
    "    \"manhattan\": lambda q, E: 1 - (pairwise_distances(q, E, metric='manhattan')[0] / pairwise_distances(q, E, metric='manhattan')[0].max())\n",
    "}\n",
    "\n",
    "precision_at_k_metrics = {name: {k: [] for k in K_VALUES} for name in similarity_methods}\n",
    "\n",
    "for i in tqdm(retrieval_sample_indices, desc=\"Retrieving recipes\", ncols=100):\n",
    "    i = int(i)\n",
    "    query = test_dataset[i]['input_text']\n",
    "\n",
    "    # Extract ingredients only\n",
    "    if \"using:\" in query.lower():\n",
    "        query_ingredients = query.split(\"using:\")[-1].strip()\n",
    "    else:\n",
    "        query_ingredients = query.strip()\n",
    "\n",
    "    # Compute query embedding\n",
    "    query_emb = np.array(get_t5_embeddings([query_ingredients], model, tokenizer))\n",
    "\n",
    "    # Compute Precision@K for each similarity method\n",
    "    for name, func in similarity_methods.items():\n",
    "        sims = func(query_emb, recipe_embeddings)\n",
    "        # Top 20 excluding self\n",
    "        all_indices = np.argsort(sims)[::-1]\n",
    "        top_indices = [idx for idx in all_indices if idx != i][:20]\n",
    "        top_sims = [sims[idx] for idx in top_indices]\n",
    "\n",
    "        for k in K_VALUES:\n",
    "            top_k_sims = top_sims[:k]\n",
    "            relevant_count = sum(1 for sim in top_k_sims if sim >= RELEVANCE_THRESHOLD)\n",
    "            precision = relevant_count / k\n",
    "            precision_at_k_metrics[name][k].append(precision)\n",
    "\n",
    "mean_precision_at_k = {\n",
    "    name: [np.mean(precision_at_k_metrics[name][k]) for k in K_VALUES]\n",
    "    for name in similarity_methods\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_table = []\n",
    "\n",
    "for name in similarity_methods:\n",
    "    for k in K_VALUES:\n",
    "        mean_val = np.mean(precision_at_k_metrics[name][k])\n",
    "        precision_table.append({\n",
    "            \"Metric\": name,\n",
    "            \"K\": k,\n",
    "            \"Mean Precision@K\": round(mean_val, 3)\n",
    "        })\n",
    "\n",
    "precision_df = pd.DataFrame(precision_table)\n",
    "\n",
    "print(precision_df.pivot(index=\"K\", columns=\"Metric\", values=\"Mean Precision@K\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of Precision@K Degradation\n",
    "plt.figure(figsize=(9, 5))\n",
    "\n",
    "colors = {\"cosine\": \"steelblue\", \"euclidean\": \"orange\", \"manhattan\": \"green\"}\n",
    "\n",
    "for name in similarity_methods:\n",
    "    plt.plot(K_VALUES, mean_precision_at_k[name],\n",
    "             marker='o', linewidth=2, markersize=6, color=colors[name], label=f'{name} (mean)')\n",
    "\n",
    "plt.axhline(RELEVANCE_THRESHOLD, color='red', linestyle='--', label=f'Threshold ({RELEVANCE_THRESHOLD})')\n",
    "\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Precision@K')\n",
    "plt.title('Similarity Metric Comparison')\n",
    "plt.xticks(K_VALUES)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
